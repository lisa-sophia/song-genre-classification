{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17300,
     "status": "ok",
     "timestamp": 1645532044623,
     "user": {
      "displayName": "Lisa Spahn Lundgren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13266833213640265739"
     },
     "user_tz": -60
    },
    "id": "VXak8PjjRAQW",
    "outputId": "66554787-38d7-4d5e-f838-a12226fd84fe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36270,
     "status": "ok",
     "timestamp": 1645532377931,
     "user": {
      "displayName": "Lisa Spahn Lundgren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13266833213640265739"
     },
     "user_tz": -60
    },
    "id": "AawEopyLSAmU",
    "outputId": "1a2cd245-6593-4af2-c426-4a11edac1a1d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>instrumental</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>country.00041.wav</td>\n",
       "      <td>Johnny Cash</td>\n",
       "      <td>Goin' By The Book</td>\n",
       "      <td>country</td>\n",
       "      <td>\\nYou can see it in the movies and the paper a...</td>\n",
       "      <td>False</td>\n",
       "      <td>['see', 'movies', 'paper', 'tv', 'news', 'some...</td>\n",
       "      <td>['see', 'movie', 'paper', 'tv', 'news', 'someb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reggae.00038.wav</td>\n",
       "      <td>Burning Spear</td>\n",
       "      <td>Investigation Dub</td>\n",
       "      <td>reggae</td>\n",
       "      <td>I and I old I know\\nI and I old I say\\nI and I...</td>\n",
       "      <td>False</td>\n",
       "      <td>['old', 'know', 'old', 'say', 'reconsider', 's...</td>\n",
       "      <td>['old', 'know', 'old', 'say', 'reconsider', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disco.00070.wav</td>\n",
       "      <td>KC and The Sunshine Band</td>\n",
       "      <td>That's The Way (I Like It)</td>\n",
       "      <td>disco</td>\n",
       "      <td>That's the way, aha, aha\\nI like it, aha, aha\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>['thats', 'way', 'aha', 'aha', 'like', 'aha', ...</td>\n",
       "      <td>['thats', 'way', 'aha', 'aha', 'like', 'aha', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reggae.00088.wav</td>\n",
       "      <td>Marcia Griffiths</td>\n",
       "      <td>Electric Boogie</td>\n",
       "      <td>reggae</td>\n",
       "      <td>\\nIt's electric!\\n\\n\\nYou can't see it\\nIt's e...</td>\n",
       "      <td>False</td>\n",
       "      <td>['electric', 'cant', 'see', 'electric', 'got',...</td>\n",
       "      <td>['electric', 'cant', 'see', 'electric', 'get',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pop.00017.wav</td>\n",
       "      <td>Britney Spears</td>\n",
       "      <td>(I Can't Get No) Satisfaction</td>\n",
       "      <td>pop</td>\n",
       "      <td>\\nI can't get no satisfaction\\nI can't get no ...</td>\n",
       "      <td>False</td>\n",
       "      <td>['cant', 'get', 'satisfaction', 'cant', 'get',...</td>\n",
       "      <td>['cant', 'get', 'satisfaction', 'cant', 'get',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename                    artist                          title  \\\n",
       "0  country.00041.wav               Johnny Cash              Goin' By The Book   \n",
       "1   reggae.00038.wav             Burning Spear              Investigation Dub   \n",
       "2    disco.00070.wav  KC and The Sunshine Band     That's The Way (I Like It)   \n",
       "3   reggae.00088.wav          Marcia Griffiths                Electric Boogie   \n",
       "4      pop.00017.wav            Britney Spears  (I Can't Get No) Satisfaction   \n",
       "\n",
       "     genre                                             lyrics  instrumental  \\\n",
       "0  country  \\nYou can see it in the movies and the paper a...         False   \n",
       "1   reggae  I and I old I know\\nI and I old I say\\nI and I...         False   \n",
       "2    disco  That's the way, aha, aha\\nI like it, aha, aha\\...         False   \n",
       "3   reggae  \\nIt's electric!\\n\\n\\nYou can't see it\\nIt's e...         False   \n",
       "4      pop  \\nI can't get no satisfaction\\nI can't get no ...         False   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  ['see', 'movies', 'paper', 'tv', 'news', 'some...   \n",
       "1  ['old', 'know', 'old', 'say', 'reconsider', 's...   \n",
       "2  ['thats', 'way', 'aha', 'aha', 'like', 'aha', ...   \n",
       "3  ['electric', 'cant', 'see', 'electric', 'got',...   \n",
       "4  ['cant', 'get', 'satisfaction', 'cant', 'get',...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  ['see', 'movie', 'paper', 'tv', 'news', 'someb...  \n",
       "1  ['old', 'know', 'old', 'say', 'reconsider', 's...  \n",
       "2  ['thats', 'way', 'aha', 'aha', 'like', 'aha', ...  \n",
       "3  ['electric', 'cant', 'see', 'electric', 'get',...  \n",
       "4  ['cant', 'get', 'satisfaction', 'cant', 'get',...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/train_data.csv')\n",
    "df_test = pd.read_csv('../data/test_data.csv')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Nf2y352cUtG"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_OWCc7hcWu1"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tJ0u5JjhcXuV"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5435/1801529876.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_lg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     )\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfyYKlfNblZZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class SentenceVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    # Vectorize a single sentence.\n",
    "    def _transform(self, tokens):\n",
    "        vec = np.zeros(nlp.vocab.vectors.shape[1])\n",
    "        for token in tokens:\n",
    "          vec = np.add(vec, nlp.vocab[token].vector)\n",
    "        return vec\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.concatenate(\n",
    "            [self._transform(row.tokenized).reshape(1, -1) for row in X.itertuples()]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KrczCMvbrzS"
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQ95KH64c68w"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2032878,
     "status": "ok",
     "timestamp": 1645536696246,
     "user": {
      "displayName": "Lisa Spahn Lundgren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13266833213640265739"
     },
     "user_tz": -60
    },
    "id": "mljiK7JjRe2S",
    "outputId": "55210d66-c415-40b2-bea3-bfcc267be3bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', SentenceVectorizer()),\n",
       "                ('Normalizing', MinMaxScaler()),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pipe = Pipeline([('vectorizer', SentenceVectorizer()), \n",
    "                 ('Normalizing',MinMaxScaler()),\n",
    "                 ('classifier', MultinomialNB())])\n",
    "pipe.fit(df_train, df_train['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 548848,
     "status": "ok",
     "timestamp": 1645537255714,
     "user": {
      "displayName": "Lisa Spahn Lundgren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13266833213640265739"
     },
     "user_tz": -60
    },
    "id": "ae2ypFNKgpuh",
    "outputId": "40e222f3-3334-4c97-dcd9-e88810c40aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.38      0.00      0.00     64391\n",
      "        male       0.51      1.00      0.68     67312\n",
      "\n",
      "    accuracy                           0.51    131703\n",
      "   macro avg       0.45      0.50      0.34    131703\n",
      "weighted avg       0.45      0.51      0.35    131703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = pipe.predict(df_test)\n",
    "print(classification_report(y_true=df_test['gender'], y_pred=pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oc1TqwDxz3w3"
   },
   "source": [
    "Bad performance, only 51% accuracy (as good as guessing)\n",
    "-> this could be because NaiveBayes has assumption that features are independent of each other. So by doing sentence embedding and adding up the features, this is not fulfilled? Or it's the normalization step that changes the data? (necessary because NaiveBayes can't take negative values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nms-buuzn64u"
   },
   "source": [
    "### Save the models to the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrHpKDRFnXsr"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#model1 = '/content/drive/MyDrive/Colab Notebooks/gender_prediction/models/NB_countVec.sav'\n",
    "#pickle.dump(pipe, open(model1, 'wb'))\n",
    "\n",
    "#model2 = '/content/drive/MyDrive/Colab Notebooks/gender_prediction/models/NB_tfIdf.sav'\n",
    "#pickle.dump(pipe_tfidf, open(model2, 'wb'))\n",
    "\n",
    "# load the models with pickle.load(open(filename, 'rb'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMgh+tFz2LwV+MV/HvYXc9Z",
   "collapsed_sections": [],
   "name": "naive-bayes_word2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
